[[slot1]]
= Slot 1: Unveiling the Potential - Integrating OpenAI with Quarkus

In this first slot, we will explore the potential of large language models like GPT-3/GPT-4 and how they can be integrated into Quarkus applications.
We will walk through the process of integration.
We will also discuss the benefits and challenges of integrating large language models into applications and explain how Quarkus integration handles these challenges.

== The triage application

In this slot, we will be looking at the code located in the `triage-application` directory.
This application receives reviews from users and classifies them as positive or negative.

It's a very simple application, but it's a good example of how to integrate OpenAI with Quarkus.

In a terminal, navigate to the `triage-application` directory and run the following command:

[source,shell]
----
$ export OPENAI_API_KEY=<your OpenAI API key>
$ ./mvnw quarkus:dev
----

This will start the application in development mode.
You can now open a browser and navigate to `http://localhost:8080` to see the frontend of the application.

The frontend is a simple form that allows you to submit a review.
When you submit a review, the application will classify it as positive or negative and display the result.

== Anatomy of the application

Let's look at the code.

First, let's look at the `TriageService` interface:

[source,java]
----
package me.escoffier.workshop.triage;

import dev.langchain4j.service.SystemMessage;
import dev.langchain4j.service.UserMessage;
import io.quarkiverse.langchain4j.RegisterAiService;

@RegisterAiService
public interface TriageService {

    @SystemMessage("""
            You are working for a bank. You are an AI processing reviews about financial products. You need to triage the reviews into positive and negative ones.
            You will always answer with a JSON document, and only this JSON document.
            """)
    @UserMessage("""
            Your task is to process the review delimited by ---.
            Apply a sentiment analysis to the passed review to determine if it is positive or negative.
            The review can be in any language. So, you will need to identify the language.

            For example:
            - "I love your bank, you are the best!", this is a 'POSITIVE' review
            - "J'adore votre banque", this is a 'POSITIVE' review
            - "I hate your bank, you are the worst!", this is a 'NEGATIVE' review

             Answer with a JSON document containing:
            - the 'evaluation' key set to 'POSITIVE' if the review is positive, 'NEGATIVE' otherwise, depending if the review is positive or negative
            - the 'message' key set to a message thanking the customer in the case of a positive review, or an apology and a note that the bank is going to contact the customer in the case of a negative review. These messages must be polite and use the same language as the passed review.

            ---
            {review}
            ---
            """)
    TriagedReview triage(String review);

}
----

This interface defines the contract between the application and the AI service.
It contains the _prompt_ that will be sent to the AI service and the expected response.

Quarkus does not use a specific client.
It proposes to use an interface and a set to annotation to model and invoke the AI service.
It encapsulates the complexity of the AI service and allows you to focus on the business logic.

The `@RegisterAiService` annotation is used to register the service.
Quarkus will use this annotation to generate the client code (at build time) and to inject the client into the application.

The `@SystemMessage` and `@UserMessage` annotations are used to define the messages (prompt) that will be sent to the AI service.
`@SystemMessage` defines the scope, the context, and the goal of the AI service.
`@UserMessage` defines the message that will be sent to the AI service.

Note that the `triage` method receives a `String` as a parameter (the user review).
The prompt can reference the parameter using the `\{review}` placeholder.

TIPS: Under the hood, Quarkus uses `qute` as template engine.

The prompt is written in a way to explain to the AI service what it needs to do and what kind of response it should return.
Thus, the `triage` method can return a `TriageReview`:

[source, java]
----
package me.escoffier.workshop.triage;

import com.fasterxml.jackson.annotation.JsonCreator;

public record TriagedReview(Evaluation evaluation, String message) {

    @JsonCreator
    public TriagedReview {
    }

}
----

The `TriageReview` is a simple record that contains the evaluation (positive or negative) and the message to send to the user.

Let's now see how we can use your AI service:

[source,java]
----
package me.escoffier.workshop.triage;

import jakarta.inject.Inject;
import jakarta.ws.rs.POST;
import jakarta.ws.rs.Path;

@Path("/review")
public class ReviewResource {

    @Inject
    TriageService triage;

    record Review(String review) {
    }

    @POST
    public TriagedReview triage(Review review) {
        return triage.triage(review.review());
    }

}
----

The `ReviewResource` is a simple JAX-RS resource that receives a `Review` and delegates the triage to the `TriageService`.
The `triage` method is called from the frontend you saw earlier.
The `TriageService` is injected by Quarkus and expose the interface defined earlier.
So, we do not leak any details about the AI service.

== Configuring the AI service

In the `application.properties` file, you will see the following:

[source,properties]
----
quarkus.langchain4j.openai.api-key=${OPENAI_API_KEY}
quarkus.langchain4j.openai.chat-model.temperature=0.5
quarkus.langchain4j.openai.timeout=60s
----

The first property is used to configure the API key.
The _temperature_ is used to control the _creative_ aspect of the AI service.
The higher the temperature, the more creative the AI service will be.
In our case, we want to limit the creativity to avoid unexpected results.

The last property is used to configure the timeout.
OpenAI can be slow to answer.
60s is generally a good value.
However, feel free to adapt.

== Under the hood

As we have seen, Quarkus integrates LLM using a declarative approach.
It models the AI service using an interface and annotations.

At build time, Quarkus generates the actual client that connect and invoke the remote model.
It uses langchain4j to manage that interaction.

NOTE: If you prefer a pure programmatic approach, you can use the langchain4j API directly in Quarkus. However, you will lose some of the benefits we are going to see in the following sections.


== Fault-Tolerance and Resilience

In this section, we will explore how Quarkus can help you to build fault-tolerant and resilient AI services.
First, check that the `pom.xml` file located in the `triage-application` directory contains the following dependency:

[source,xml]
----
<dependency>
    <groupId>io.quarkus</groupId>
    <artifactId>quarkus-smallrye-fault-tolerance</artifactId>
</dependency>
----

This Quarkus _extension_ provides a set of annotations (as well as a programmatic API) to express the fault-tolerance and resilience requirements of your application.
Let's extend our AI service to make it more resilient.

Open the `TriageService` interface and add (if not already present) the following annotation to the `triage` method:

[source,java]
----
// Do not forget to add the following import:
// import org.eclipse.microprofile.faulttolerance.Fallback;
// import org.eclipse.microprofile.faulttolerance.Retry;

@Retry(maxRetries = 2)
@Fallback(fallbackMethod = "fallback")
TriagedReview triage(String review);
----

The `@Retry` annotation is used to retry the invocation of the AI service in case of failure.
In this case, we will retry twice.
The `@Fallback` annotation is used to define a fallback method that will be invoked if the AI service failed to answer (after the 2 retries).

Let's implement the fallback method:

[source, java]
----
static TriagedReview fallback(String review) {
    return new TriagedReview(Evaluation.NEGATIVE, "Sorry, we are unable to process your review at the moment. Please try again later.");
}
----

The fallback method returns a negative evaluation and a message explaining that the service is unavailable.

The Quarkus fault-tolerance support also provides timeout, circuit breaker, bulkhead and rate limiting.
Check https://quarkus.io/guides/smallrye-fault-tolerance[the Quarkus documentation] for more details.

== Observability

In this section, we will explore how Quarkus can help you to monitor and observe your AI services.
First, check that the `pom.xml` file located in the `triage-application` directory contains the following dependencies:

[source,xml]
----
<dependency>
    <groupId>io.quarkus</groupId>
    <artifactId>quarkus-micrometer-registry-prometheus</artifactId>
</dependency>
<dependency>
    <groupId>io.quarkus</groupId>
    <artifactId>quarkus-opentelemetry</artifactId>
</dependency>
----

The first dependency is used to expose metrics using the Prometheus format.
Quarkus metrics are based on https://micrometer.io/[Micrometer].

The second dependency is used to expose traces using the OpenTelemetry format.

=== Metrics

With the `quarkus-micrometer-registry-prometheus`, Quarkus will automatically expose metrics and traces for your application.
It also provides specific metrics for the AI service.
For example, the number of requests, the number of errors, the response time, etc.

Start the application and post a few reviews.
Then, open a browser and navigate to `http://localhost:8080/q/metrics` to see the metrics:

[source, text]
----
# HELP langchain4j_aiservices_TriageService_triage_seconds
# TYPE langchain4j_aiservices_TriageService_triage_seconds summary
langchain4j_aiservices_TriageService_triage_seconds_count 2.0
langchain4j_aiservices_TriageService_triage_seconds_sum 4.992278791
# HELP langchain4j_aiservices_TriageService_triage_seconds_max
# TYPE langchain4j_aiservices_TriageService_triage_seconds_max gauge
langchain4j_aiservices_TriageService_triage_seconds_max 2.706755083
----

You can see that the `triage` method has been invoked twice and that the response time sum was 4.992278791 seconds.
The max duration os the call was 2.706755083 seconds.

=== Tracing

With the `quarkus-opentelemetry` extension, Quarkus will automatically expose traces for your application.
The Quarkus tracing support is based on https://opentelemetry.io/[OpenTelemetry].

Before seeing traces, we must start the OpenTelemetry collector and the Jaeger UI (to visualize the traces).
In a terminal, navigate to the `triage-application` directory and run the following command:

[source, shell]
----
$ docker-compose -f observability-stack.yml up
----

With the observability stack up, we can start submitting reviews for triage.
Then, open a browser and navigate to `http://localhost:16686` to see the traces.

* Select the `quarkus-llm-workshop-triage` service
* Click on the `Find Traces` button

You will see the trace on the right side of the screen.
If you click on one, you will see the details of the trace.
Quarkus instruments the AI service and the application to provide a complete trace.
Under that trace, you will see the trace of the actual call to the model (`POST`):

image::../images/trace.png[width=1024]

== Summary

This concludes the first slots.
In this slot, we have seen how Quarkus can help you to integrate OpenAI with your application.
Quarkus provides a declarative approach to integrate LLMs.
The interactions are model in a Java interface containing methods annotated with `@SystemMessage` and `@UserMessage`.
At build time, Quarkus generates the actual client code.
We have also discussed how fault-tolerance, metrics and tracing have been added.
